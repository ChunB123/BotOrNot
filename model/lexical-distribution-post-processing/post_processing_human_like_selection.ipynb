{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning"
   ],
   "metadata": {
    "id": "l583u0ix5UyF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install datasets"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jk_8yRi10T8C",
    "outputId": "8c6bcfac-1b04-4107-eb25-ff9f40ba7932"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m480.6/480.6 kB\u001B[0m \u001B[31m6.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m4.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m179.3/179.3 kB\u001B[0m \u001B[31m3.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"AlanYky/tweets_instruct_2\", split=\"train\")"
   ],
   "metadata": {
    "id": "tm-9vySZ0TMO"
   },
   "execution_count": 82,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = dataset.to_pandas()\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Id6oGRYe7Rd0",
    "outputId": "3d31e69d-1f5f-44e9-f9bd-9d65928eac13"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             instruction  \\\n",
       "0                           Generate a tweet about FAMU.   \n",
       "1                            Generate a tweet about dbh.   \n",
       "2                         Generate a tweet about Madrid.   \n",
       "3                      Generate a tweet about Patronato.   \n",
       "4                   Generate a tweet about Torrey Pines.   \n",
       "...                                                  ...   \n",
       "200453       Can you write a tweet with a friendly tone?   \n",
       "200454  Can you write a tweet with a controversial tone?   \n",
       "200455  Can you write a tweet with a controversial tone?   \n",
       "200456  Can you write a tweet with a controversial tone?   \n",
       "200457  Can you write a tweet with a controversial tone?   \n",
       "\n",
       "                                                     text  \n",
       "0       School Monday and honestly I've always hated s...  \n",
       "1       dbh the worst game of all time its awful its t...  \n",
       "2       I wish I had a @user cheki to do that wota thi...  \n",
       "3       10‚Äô | 0-0 | Good start, Patronato dangerous on...  \n",
       "4       I don't have a big opinion on the Torrey Pines...  \n",
       "...                                                   ...  \n",
       "200453  You're eating skin that could have been sent t...  \n",
       "200454  Very important thing for today: \\n\\nDo not #bu...  \n",
       "200455  Which #chutiya #producer #invested in #crap #d...  \n",
       "200456  Russia story will infuriate Trump today. Media...  \n",
       "200457                        Shit getting me irritated üò†  \n",
       "\n",
       "[200458 rows x 2 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-4318694c-314b-45b7-93ab-d1dda5de6d59\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generate a tweet about FAMU.</td>\n",
       "      <td>School Monday and honestly I've always hated s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generate a tweet about dbh.</td>\n",
       "      <td>dbh the worst game of all time its awful its t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Generate a tweet about Madrid.</td>\n",
       "      <td>I wish I had a @user cheki to do that wota thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Generate a tweet about Patronato.</td>\n",
       "      <td>10‚Äô | 0-0 | Good start, Patronato dangerous on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generate a tweet about Torrey Pines.</td>\n",
       "      <td>I don't have a big opinion on the Torrey Pines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200453</th>\n",
       "      <td>Can you write a tweet with a friendly tone?</td>\n",
       "      <td>You're eating skin that could have been sent t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200454</th>\n",
       "      <td>Can you write a tweet with a controversial tone?</td>\n",
       "      <td>Very important thing for today: \\n\\nDo not #bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200455</th>\n",
       "      <td>Can you write a tweet with a controversial tone?</td>\n",
       "      <td>Which #chutiya #producer #invested in #crap #d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200456</th>\n",
       "      <td>Can you write a tweet with a controversial tone?</td>\n",
       "      <td>Russia story will infuriate Trump today. Media...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200457</th>\n",
       "      <td>Can you write a tweet with a controversial tone?</td>\n",
       "      <td>Shit getting me irritated üò†</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200458 rows √ó 2 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4318694c-314b-45b7-93ab-d1dda5de6d59')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-4318694c-314b-45b7-93ab-d1dda5de6d59 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-4318694c-314b-45b7-93ab-d1dda5de6d59');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-1e287e74-ef3b-44ee-9187-8eb849616092\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1e287e74-ef3b-44ee-9187-8eb849616092')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-1e287e74-ef3b-44ee-9187-8eb849616092 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_637896be-b358-428e-934b-d0f713c86528\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_637896be-b358-428e-934b-d0f713c86528 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('df');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df"
      }
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the Noise Signal\n",
    "def print_detection(data):\n",
    "    num_rows = len(data)\n",
    "    print(\"Number of data: \", num_rows)\n",
    "    print(\"-----\")\n",
    "    link_count = data['text'].str.contains(r\"http\\S+|www\\.\\S+\", case=False).sum()\n",
    "    mention_count = data['text'].str.contains(r\"@\\w+\").sum()\n",
    "    hashtag_count = data['text'].str.contains(r\"#\\w+\").sum()\n",
    "    long_text_count = data['text'].str.split().apply(len).gt(250).sum()\n",
    "\n",
    "    multiple_mentions_count = data['text'].str.count(r\"@\\w+\").gt(1).sum()\n",
    "    multiple_hashtags_count = data['text'].str.count(r\"#\\w+\").gt(2).sum()\n",
    "    # low_quality_content_count = data['text'].str.contains(\"click\", case=False).sum()\n",
    "    low_quality_content_count = data['text'].str.contains(r\"click|sale|subscribe|link in bio|RT\", case=False).sum()\n",
    "\n",
    "    print(\"link_count\", link_count)\n",
    "    print(\"mention_count\", mention_count)\n",
    "    print(\"hashtag_count\", hashtag_count)\n",
    "    print(\"long_text_count\", long_text_count)\n",
    "    print(\"Texts with more than 1 mention:\", multiple_mentions_count)\n",
    "    print(\"Texts with more than 2 hashtag:\", multiple_hashtags_count)\n",
    "    print(\"low_quality_content_count\", low_quality_content_count)\n",
    "\n",
    "# Remove the Link Data\n",
    "# Delete rows containing links in the text column\n",
    "def remove_links(dataframe):\n",
    "    dataframe = dataframe[~dataframe['text'].str.contains(r\"http\\S+|www\\.\\S+\", case=False, regex=True)]\n",
    "    return dataframe\n",
    "\n",
    "# Remove the Extreme Long Data\n",
    "def remove_long_texts(dataframe, word_limit=250):\n",
    "    return dataframe[~dataframe['text'].str.split().apply(len).gt(word_limit)]\n",
    "\n",
    "# Remove the data have mentions @\n",
    "def remove_mentions(dataframe):\n",
    "    return dataframe[~dataframe['text'].str.contains(r\"@\\w+\")]\n",
    "\n",
    "# Remove the low quality data (with too advertisement text)\n",
    "def remove_low_quality_content(dataframe):\n",
    "    low_quality_pattern = r\"click|subscribe|link in bio|RT\"\n",
    "    dataframe = dataframe[~dataframe['text'].str.contains(low_quality_pattern, case=False, regex=True)]\n",
    "    return dataframe"
   ],
   "metadata": {
    "id": "Eof9681l7D5i"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print_detection(df)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lqtLbbbD7EAk",
    "outputId": "5975c72d-cc6e-42c5-b466-f045471bc67a"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of data:  200458\n",
      "-----\n",
      "link_count 3258\n",
      "mention_count 930\n",
      "hashtag_count 27195\n",
      "long_text_count 0\n",
      "Texts with more than 1 mention: 4\n",
      "Texts with more than 2 hashtag: 5412\n",
      "low_quality_content_count 28829\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "df = remove_links(df)\n",
    "df = remove_long_texts(df)\n",
    "df = remove_mentions(df)\n",
    "df = remove_low_quality_content(df)"
   ],
   "metadata": {
    "id": "FP58pIFr7EHo"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print_detection(df)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWnwfjHg7EOB",
    "outputId": "12e25c17-8c93-457a-ffb5-ea498e14c850"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of data:  168881\n",
      "-----\n",
      "link_count 0\n",
      "mention_count 0\n",
      "hashtag_count 20225\n",
      "long_text_count 0\n",
      "Texts with more than 1 mention: 0\n",
      "Texts with more than 2 hashtag: 3637\n",
      "low_quality_content_count 425\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Benchmark Human Tweets POS and Constituent Distribution"
   ],
   "metadata": {
    "id": "kjbn-HPp5cj-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "from collections import Counter\n",
    "from nltk.tree import Tree\n",
    "from nltk.corpus import treebank\n",
    "import re"
   ],
   "metadata": {
    "id": "kT-k-f9vxx48"
   },
   "execution_count": 116,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('treebank')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83YQ3upC8SoC",
    "outputId": "89b0ea30-5796-4821-e656-bc1327552bbe"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Define grammar for chunking\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN.*>}    # Noun phrase\n",
    "    VP: {<VB.*><NP|PP|CLAUSE>+$}  # Verb phrase\n",
    "    PP: {<IN><NP>}              # Prepositional phrase\n",
    "    ADJP: {<JJ><CC>*<JJ>*}      # Adjective phrase\n",
    "    ADVP: {<RB.*>}              # Adverb phrase\n",
    "\"\"\"\n",
    "\n",
    "# Create a RegexpParser with the grammar\n",
    "parser = RegexpParser(grammar)"
   ],
   "metadata": {
    "id": "8-bBTNO38n-n"
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Main Pipeline"
   ],
   "metadata": {
    "id": "uk2XeiuvOGOl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "human_distribution_benchmark = {\n",
    "   \"NNP\":0.11107245214850336,\n",
    "   \"CC\":0.024262175839871215,\n",
    "   \"RB\":0.05238026804527889,\n",
    "   \"PRP\":0.05956264107107177,\n",
    "   \"VBP\":0.0363054932074732,\n",
    "   \"VBN\":0.011839412350151873,\n",
    "   \"NN\":0.1409238945795302,\n",
    "   \"VB\":0.04371257145342362,\n",
    "   \"DT\":0.05979932216551541,\n",
    "   \"IN\":0.07728031676817795,\n",
    "   \"CD\":0.017169621871224054,\n",
    "   \"NNS\":0.030351277605178115,\n",
    "   \".\":0.06337695703119336,\n",
    "   \"JJ\":0.058788306252099715,\n",
    "   \"''\":0.006478002525018547,\n",
    "   \",\":0.01874570995819159,\n",
    "   \"WDT\":0.0016583434340378298,\n",
    "   \"VBZ\":0.025435181210735677,\n",
    "   \"POS\":0.003736787931848451,\n",
    "   \"TO\":0.022256216896950124,\n",
    "   \"``\":0.0035788954840238686,\n",
    "   \"PRP$\":0.015399398558987169,\n",
    "   \"VBD\":0.02242545490988785,\n",
    "   \"JJS\":0.0018401876304346045,\n",
    "   \"VBG\":0.021872358610622096,\n",
    "   \"MD\":0.012826161360408895,\n",
    "   \"#\":0.011843194205189587,\n",
    "   \"WRB\":0.005464150220324571,\n",
    "   \"WP\":0.0038455162641827445,\n",
    "   \"PDT\":0.0007850500749122452,\n",
    "   \"RP\":0.005637485242886488,\n",
    "   \"RBR\":0.0009126876824351113,\n",
    "   \"JJR\":0.002109644801871766,\n",
    "   \"$\":0.000583035984980993,\n",
    "   \":\":0.0169212800570808,\n",
    "   \"(\":0.002084432434953669,\n",
    "   \")\":0.0024471753639877898,\n",
    "   \"EX\":0.0009987248845431172,\n",
    "   \"FW\":0.00040875549865964755,\n",
    "   \"NNPS\":0.0011373929025926507,\n",
    "   \"RBS\":0.00027985727279087664,\n",
    "   \"UH\":0.001336570601245617,\n",
    "   \"WP$\":2.4897212331620784e-05,\n",
    "   \"SYM\":9.265544842400646e-05,\n",
    "   \"LS\":1.0084946767238798e-05,\n",
    "   \"NP\":0.6919773154224048,\n",
    "   \"ADVP\":0.1307694201308702,\n",
    "   \"ADJP\":0.05708685343799667,\n",
    "   \"PP\":0.11067809101217466,\n",
    "   \"VP\":0.009488319996553618\n",
    "}\n",
    "\n",
    "def calculate_distributions(texts):\n",
    "    pos_counts = Counter()\n",
    "    constituent_counts = Counter()\n",
    "    total_tokens = 0\n",
    "    total_constituents = 0\n",
    "\n",
    "    for i, text in enumerate(texts, start=1):\n",
    "        # Tokenize and POS tag\n",
    "        tokens = word_tokenize(text)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "\n",
    "        # Calculate POS distribution\n",
    "        for _, tag in pos_tags:\n",
    "            pos_counts[tag] += 1\n",
    "            total_tokens += 1\n",
    "\n",
    "        # Parse for constituent distribution\n",
    "        parse_tree = parser.parse(pos_tags)\n",
    "        for subtree in parse_tree.subtrees():\n",
    "            if subtree.label() in {\"NP\", \"VP\", \"PP\", \"ADJP\", \"ADVP\"}:\n",
    "                constituent_counts[subtree.label()] += 1\n",
    "                total_constituents += 1\n",
    "\n",
    "        # Print progress every 1000 texts\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Finished processing {i} texts\")\n",
    "\n",
    "    # Convert counts to percentages\n",
    "    pos_distribution = {pos: count / total_tokens for pos, count in pos_counts.items()}\n",
    "    constituent_distribution = {constituent: count / total_constituents\n",
    "                                for constituent, count in constituent_counts.items()}\n",
    "\n",
    "    # Combine both distributions in one dictionary\n",
    "    combined_distribution = {**pos_distribution, **constituent_distribution}\n",
    "\n",
    "    return combined_distribution\n",
    "\n",
    "\n",
    "def cosine_similarity(dist1, dist2):\n",
    "    # Calculate cosine similarity between two distributions\n",
    "    all_keys = set(dist1.keys()).union(dist2.keys())\n",
    "    vec1 = [dist1.get(key, 0) for key in all_keys]\n",
    "    vec2 = [dist2.get(key, 0) for key in all_keys]\n",
    "\n",
    "    dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "    magnitude1 = math.sqrt(sum(a * a for a in vec1))\n",
    "    magnitude2 = math.sqrt(sum(b * b for b in vec2))\n",
    "\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0  # Avoid division by zero\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "\n",
    "def remove_words_with_ampersand(input_text):\n",
    "    # Split the text into words\n",
    "    words = input_text.split()\n",
    "    # Filter out words containing '&'\n",
    "    filtered_words = [word for word in words if '&' not in word]\n",
    "    # Join the remaining words back into a string\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # Remove trailing sequences of non-ASCII characters (like `ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ`)\n",
    "    cleaned_tweet = re.sub(r\"[^\\x00-\\x7F]+$\", \"\", tweet)\n",
    "\n",
    "    # change more than one dot to one dot\n",
    "    cleaned_tweet = re.sub(r\"\\.{2,}\", \".\", cleaned_tweet)\n",
    "\n",
    "    # change more than two question marks to two question marks\n",
    "    cleaned_tweet = re.sub(r\"\\?{3,}\", \"??\", cleaned_tweet)\n",
    "\n",
    "    # change more than two ! to two !\n",
    "    cleaned_tweet = re.sub(r\"!{3,}\", \"!!\", cleaned_tweet)\n",
    "\n",
    "    # remove the word including &\n",
    "    cleaned_tweet = remove_words_with_ampersand(cleaned_tweet)\n",
    "\n",
    "    return cleaned_tweet.strip()\n",
    "\n",
    "def most_human_like_tweet(generated_tweets, benchmark):\n",
    "    best_score = -1\n",
    "    best_tweet = None\n",
    "\n",
    "    for tweet in generated_tweets:\n",
    "        # Calculate the distribution for the generated tweet\n",
    "        cleaned_tweet = clean_tweet(tweet)\n",
    "        tweet_distribution = calculate_distributions([cleaned_tweet])\n",
    "\n",
    "        # Calculate similarity with the benchmark\n",
    "        score = cosine_similarity(tweet_distribution, benchmark)\n",
    "\n",
    "        print(f\"Tweet: {cleaned_tweet}\")\n",
    "        print(f\"Human-likeness Score: {score:.4f}\")\n",
    "\n",
    "        # Check if this is the highest score\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_tweet = cleaned_tweet\n",
    "\n",
    "    return best_tweet"
   ],
   "metadata": {
    "id": "PNVtPK1T80PR"
   },
   "execution_count": 121,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Extract the 'text' column and select only the first 10 tweets\n",
    "human_tweets = dataset[\"text\"]\n",
    "\n",
    "# Calculate POS and constituent type distributions for the first 10 human tweets\n",
    "distribution = calculate_distributions(human_tweets)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAtStV6n2WtV",
    "outputId": "50527b32-cdbc-45ba-e085-fb806ecbf0c2"
   },
   "execution_count": 86,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Finished processing 1000 texts\n",
      "Finished processing 2000 texts\n",
      "Finished processing 3000 texts\n",
      "Finished processing 4000 texts\n",
      "Finished processing 5000 texts\n",
      "Finished processing 6000 texts\n",
      "Finished processing 7000 texts\n",
      "Finished processing 8000 texts\n",
      "Finished processing 9000 texts\n",
      "Finished processing 10000 texts\n",
      "Finished processing 11000 texts\n",
      "Finished processing 12000 texts\n",
      "Finished processing 13000 texts\n",
      "Finished processing 14000 texts\n",
      "Finished processing 15000 texts\n",
      "Finished processing 16000 texts\n",
      "Finished processing 17000 texts\n",
      "Finished processing 18000 texts\n",
      "Finished processing 19000 texts\n",
      "Finished processing 20000 texts\n",
      "Finished processing 21000 texts\n",
      "Finished processing 22000 texts\n",
      "Finished processing 23000 texts\n",
      "Finished processing 24000 texts\n",
      "Finished processing 25000 texts\n",
      "Finished processing 26000 texts\n",
      "Finished processing 27000 texts\n",
      "Finished processing 28000 texts\n",
      "Finished processing 29000 texts\n",
      "Finished processing 30000 texts\n",
      "Finished processing 31000 texts\n",
      "Finished processing 32000 texts\n",
      "Finished processing 33000 texts\n",
      "Finished processing 34000 texts\n",
      "Finished processing 35000 texts\n",
      "Finished processing 36000 texts\n",
      "Finished processing 37000 texts\n",
      "Finished processing 38000 texts\n",
      "Finished processing 39000 texts\n",
      "Finished processing 40000 texts\n",
      "Finished processing 41000 texts\n",
      "Finished processing 42000 texts\n",
      "Finished processing 43000 texts\n",
      "Finished processing 44000 texts\n",
      "Finished processing 45000 texts\n",
      "Finished processing 46000 texts\n",
      "Finished processing 47000 texts\n",
      "Finished processing 48000 texts\n",
      "Finished processing 49000 texts\n",
      "Finished processing 50000 texts\n",
      "Finished processing 51000 texts\n",
      "Finished processing 52000 texts\n",
      "Finished processing 53000 texts\n",
      "Finished processing 54000 texts\n",
      "Finished processing 55000 texts\n",
      "Finished processing 56000 texts\n",
      "Finished processing 57000 texts\n",
      "Finished processing 58000 texts\n",
      "Finished processing 59000 texts\n",
      "Finished processing 60000 texts\n",
      "Finished processing 61000 texts\n",
      "Finished processing 62000 texts\n",
      "Finished processing 63000 texts\n",
      "Finished processing 64000 texts\n",
      "Finished processing 65000 texts\n",
      "Finished processing 66000 texts\n",
      "Finished processing 67000 texts\n",
      "Finished processing 68000 texts\n",
      "Finished processing 69000 texts\n",
      "Finished processing 70000 texts\n",
      "Finished processing 71000 texts\n",
      "Finished processing 72000 texts\n",
      "Finished processing 73000 texts\n",
      "Finished processing 74000 texts\n",
      "Finished processing 75000 texts\n",
      "Finished processing 76000 texts\n",
      "Finished processing 77000 texts\n",
      "Finished processing 78000 texts\n",
      "Finished processing 79000 texts\n",
      "Finished processing 80000 texts\n",
      "Finished processing 81000 texts\n",
      "Finished processing 82000 texts\n",
      "Finished processing 83000 texts\n",
      "Finished processing 84000 texts\n",
      "Finished processing 85000 texts\n",
      "Finished processing 86000 texts\n",
      "Finished processing 87000 texts\n",
      "Finished processing 88000 texts\n",
      "Finished processing 89000 texts\n",
      "Finished processing 90000 texts\n",
      "Finished processing 91000 texts\n",
      "Finished processing 92000 texts\n",
      "Finished processing 93000 texts\n",
      "Finished processing 94000 texts\n",
      "Finished processing 95000 texts\n",
      "Finished processing 96000 texts\n",
      "Finished processing 97000 texts\n",
      "Finished processing 98000 texts\n",
      "Finished processing 99000 texts\n",
      "Finished processing 100000 texts\n",
      "Finished processing 101000 texts\n",
      "Finished processing 102000 texts\n",
      "Finished processing 103000 texts\n",
      "Finished processing 104000 texts\n",
      "Finished processing 105000 texts\n",
      "Finished processing 106000 texts\n",
      "Finished processing 107000 texts\n",
      "Finished processing 108000 texts\n",
      "Finished processing 109000 texts\n",
      "Finished processing 110000 texts\n",
      "Finished processing 111000 texts\n",
      "Finished processing 112000 texts\n",
      "Finished processing 113000 texts\n",
      "Finished processing 114000 texts\n",
      "Finished processing 115000 texts\n",
      "Finished processing 116000 texts\n",
      "Finished processing 117000 texts\n",
      "Finished processing 118000 texts\n",
      "Finished processing 119000 texts\n",
      "Finished processing 120000 texts\n",
      "Finished processing 121000 texts\n",
      "Finished processing 122000 texts\n",
      "Finished processing 123000 texts\n",
      "Finished processing 124000 texts\n",
      "Finished processing 125000 texts\n",
      "Finished processing 126000 texts\n",
      "Finished processing 127000 texts\n",
      "Finished processing 128000 texts\n",
      "Finished processing 129000 texts\n",
      "Finished processing 130000 texts\n",
      "Finished processing 131000 texts\n",
      "Finished processing 132000 texts\n",
      "Finished processing 133000 texts\n",
      "Finished processing 134000 texts\n",
      "Finished processing 135000 texts\n",
      "Finished processing 136000 texts\n",
      "Finished processing 137000 texts\n",
      "Finished processing 138000 texts\n",
      "Finished processing 139000 texts\n",
      "Finished processing 140000 texts\n",
      "Finished processing 141000 texts\n",
      "Finished processing 142000 texts\n",
      "Finished processing 143000 texts\n",
      "Finished processing 144000 texts\n",
      "Finished processing 145000 texts\n",
      "Finished processing 146000 texts\n",
      "Finished processing 147000 texts\n",
      "Finished processing 148000 texts\n",
      "Finished processing 149000 texts\n",
      "Finished processing 150000 texts\n",
      "Finished processing 151000 texts\n",
      "Finished processing 152000 texts\n",
      "Finished processing 153000 texts\n",
      "Finished processing 154000 texts\n",
      "Finished processing 155000 texts\n",
      "Finished processing 156000 texts\n",
      "Finished processing 157000 texts\n",
      "Finished processing 158000 texts\n",
      "Finished processing 159000 texts\n",
      "Finished processing 160000 texts\n",
      "Finished processing 161000 texts\n",
      "Finished processing 162000 texts\n",
      "Finished processing 163000 texts\n",
      "Finished processing 164000 texts\n",
      "Finished processing 165000 texts\n",
      "Finished processing 166000 texts\n",
      "Finished processing 167000 texts\n",
      "Finished processing 168000 texts\n",
      "Finished processing 169000 texts\n",
      "Finished processing 170000 texts\n",
      "Finished processing 171000 texts\n",
      "Finished processing 172000 texts\n",
      "Finished processing 173000 texts\n",
      "Finished processing 174000 texts\n",
      "Finished processing 175000 texts\n",
      "Finished processing 176000 texts\n",
      "Finished processing 177000 texts\n",
      "Finished processing 178000 texts\n",
      "Finished processing 179000 texts\n",
      "Finished processing 180000 texts\n",
      "Finished processing 181000 texts\n",
      "Finished processing 182000 texts\n",
      "Finished processing 183000 texts\n",
      "Finished processing 184000 texts\n",
      "Finished processing 185000 texts\n",
      "Finished processing 186000 texts\n",
      "Finished processing 187000 texts\n",
      "Finished processing 188000 texts\n",
      "Finished processing 189000 texts\n",
      "Finished processing 190000 texts\n",
      "Finished processing 191000 texts\n",
      "Finished processing 192000 texts\n",
      "Finished processing 193000 texts\n",
      "Finished processing 194000 texts\n",
      "Finished processing 195000 texts\n",
      "Finished processing 196000 texts\n",
      "Finished processing 197000 texts\n",
      "Finished processing 198000 texts\n",
      "Finished processing 199000 texts\n",
      "Finished processing 200000 texts\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Display the distributions\n",
    "print(\"Lexical Distribution for the Human Tweets:\")\n",
    "print(distribution)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Jw-bm0L2XNN",
    "outputId": "fa29e192-cfe1-46c7-cb20-64b14b76a78e"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lexical Distribution for the Human Tweets:\n",
      "{'POS_Distribution': {'NNP': 0.11107245214850336, 'CC': 0.024262175839871215, 'RB': 0.05238026804527889, 'PRP': 0.05956264107107177, 'VBP': 0.0363054932074732, 'VBN': 0.011839412350151873, 'NN': 0.1409238945795302, 'VB': 0.04371257145342362, 'DT': 0.05979932216551541, 'IN': 0.07728031676817795, 'CD': 0.017169621871224054, 'NNS': 0.030351277605178115, '.': 0.06337695703119336, 'JJ': 0.058788306252099715, \"''\": 0.006478002525018547, ',': 0.01874570995819159, 'WDT': 0.0016583434340378298, 'VBZ': 0.025435181210735677, 'POS': 0.003736787931848451, 'TO': 0.022256216896950124, '``': 0.0035788954840238686, 'PRP$': 0.015399398558987169, 'VBD': 0.02242545490988785, 'JJS': 0.0018401876304346045, 'VBG': 0.021872358610622096, 'MD': 0.012826161360408895, '#': 0.011843194205189587, 'WRB': 0.005464150220324571, 'WP': 0.0038455162641827445, 'PDT': 0.0007850500749122452, 'RP': 0.005637485242886488, 'RBR': 0.0009126876824351113, 'JJR': 0.002109644801871766, '$': 0.000583035984980993, ':': 0.0169212800570808, '(': 0.002084432434953669, ')': 0.0024471753639877898, 'EX': 0.0009987248845431172, 'FW': 0.00040875549865964755, 'NNPS': 0.0011373929025926507, 'RBS': 0.00027985727279087664, 'UH': 0.001336570601245617, 'WP$': 2.4897212331620784e-05, 'SYM': 9.265544842400646e-05, 'LS': 1.0084946767238798e-05}, 'Constituent_Distribution': {'NP': 0.6919773154224048, 'ADVP': 0.1307694201308702, 'ADJP': 0.05708685343799667, 'PP': 0.11067809101217466, 'VP': 0.009488319996553618}}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Display the distributions\n",
    "print(\"Lexical Distribution for the Human Tweets:\")\n",
    "print(distribution)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8feHttBsDDzM",
    "outputId": "20444828-1e4b-484b-afb5-9e8def632794"
   },
   "execution_count": 87,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lexical Distribution for the Human Tweets:\n",
      "{'NNP': 0.11784279716897358, 'CC': 0.024218400664049265, 'RB': 0.05018358342244844, 'PRP': 0.056776967869437235, 'VBP': 0.03452376004040725, 'VBN': 0.011762690558332924, 'NN': 0.14094017384688828, 'VB': 0.04239662332460446, 'DT': 0.060314617271074934, 'IN': 0.07831549141786862, 'CD': 0.01759075570872018, 'NNS': 0.030624108061673955, '.': 0.06126383488450304, 'JJ': 0.05860965315014038, \"''\": 0.006541993684478352, ',': 0.01911081385081816, 'WDT': 0.0016926202613119507, 'VBZ': 0.02524848315378056, 'POS': 0.00376336569187963, 'TO': 0.022043614130948196, '``': 0.003658065011833731, 'PRP$': 0.015349715398461049, 'VBD': 0.02209576063996614, 'JJS': 0.0018689611130634563, 'WRB': 0.005254705466692362, 'MD': 0.012410617230768455, '#': 0.013422561804319595, 'VBG': 0.02162694588980964, 'WP': 0.0037767172135122437, 'PDT': 0.000785472536801706, 'RP': 0.005473116207361727, 'RBR': 0.0009023613299627039, 'EX': 0.000996325812396006, '(': 0.002306538340909692, ')': 0.002655693227377673, ':': 0.017687491261681006, 'JJR': 0.0020518517678800178, '$': 0.0006375981368329435, 'FW': 0.00042221038219360463, 'SYM': 9.421639793580438e-05, 'NNPS': 0.0012192710321104095, 'RBS': 0.0003020466875000787, 'UH': 0.001199873538417744, 'WP$': 2.8718367285245185e-05, 'LS': 8.817042587575275e-06, 'NP': 0.7006743969661854, 'ADVP': 0.12389189727593185, 'ADJP': 0.05483301629873648, 'PP': 0.11205776103122488, 'VP': 0.008542928427921394}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Post-Processing Human-like Tweet Selection based on lexical distribution"
   ],
   "metadata": {
    "id": "EMWv7VAr-w-1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import math"
   ],
   "metadata": {
    "id": "nUN7Zsk0Bzz0"
   },
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "human_distribution_benchmark = {\n",
    "   \"NNP\":0.11107245214850336,\n",
    "   \"CC\":0.024262175839871215,\n",
    "   \"RB\":0.05238026804527889,\n",
    "   \"PRP\":0.05956264107107177,\n",
    "   \"VBP\":0.0363054932074732,\n",
    "   \"VBN\":0.011839412350151873,\n",
    "   \"NN\":0.1409238945795302,\n",
    "   \"VB\":0.04371257145342362,\n",
    "   \"DT\":0.05979932216551541,\n",
    "   \"IN\":0.07728031676817795,\n",
    "   \"CD\":0.017169621871224054,\n",
    "   \"NNS\":0.030351277605178115,\n",
    "   \".\":0.06337695703119336,\n",
    "   \"JJ\":0.058788306252099715,\n",
    "   \"''\":0.006478002525018547,\n",
    "   \",\":0.01874570995819159,\n",
    "   \"WDT\":0.0016583434340378298,\n",
    "   \"VBZ\":0.025435181210735677,\n",
    "   \"POS\":0.003736787931848451,\n",
    "   \"TO\":0.022256216896950124,\n",
    "   \"``\":0.0035788954840238686,\n",
    "   \"PRP$\":0.015399398558987169,\n",
    "   \"VBD\":0.02242545490988785,\n",
    "   \"JJS\":0.0018401876304346045,\n",
    "   \"VBG\":0.021872358610622096,\n",
    "   \"MD\":0.012826161360408895,\n",
    "   \"#\":0.011843194205189587,\n",
    "   \"WRB\":0.005464150220324571,\n",
    "   \"WP\":0.0038455162641827445,\n",
    "   \"PDT\":0.0007850500749122452,\n",
    "   \"RP\":0.005637485242886488,\n",
    "   \"RBR\":0.0009126876824351113,\n",
    "   \"JJR\":0.002109644801871766,\n",
    "   \"$\":0.000583035984980993,\n",
    "   \":\":0.0169212800570808,\n",
    "   \"(\":0.002084432434953669,\n",
    "   \")\":0.0024471753639877898,\n",
    "   \"EX\":0.0009987248845431172,\n",
    "   \"FW\":0.00040875549865964755,\n",
    "   \"NNPS\":0.0011373929025926507,\n",
    "   \"RBS\":0.00027985727279087664,\n",
    "   \"UH\":0.001336570601245617,\n",
    "   \"WP$\":2.4897212331620784e-05,\n",
    "   \"SYM\":9.265544842400646e-05,\n",
    "   \"LS\":1.0084946767238798e-05,\n",
    "   \"NP\":0.6919773154224048,\n",
    "   \"ADVP\":0.1307694201308702,\n",
    "   \"ADJP\":0.05708685343799667,\n",
    "   \"PP\":0.11067809101217466,\n",
    "   \"VP\":0.009488319996553618\n",
    "}"
   ],
   "metadata": {
    "id": "1P503e2q-4W2"
   },
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generated_tweets = [\n",
    "    \"I don‚Äôt know how many times people have told me ‚ÄúChatGPT is going to take over the world‚Äù and now it seems like they were right. #chatgptiscontrollingus üò≠üòÇü§£üíÄ‚ùóÔ∏è#fuckyouallofu\",\n",
    "    \"I'm not sure if this was an accident or intentional but the last few days have been filled with people saying that ChatGpt and other AI are taking over. It seems like it has become more of our reality than we thought possible.. #TheMatrixRebornIsReal ü§Ø‚ùì\",\n",
    "    \"I'm not sure if it was the Chaos or something else, but there were some really weird things happening to me lately. It felt like my brain had been hacked and someone took control of what i thought/felt - especially when writing this article for example: https://tapas-hubbard\",\n",
    "]"
   ],
   "metadata": {
    "id": "smniqw9PE6HX"
   },
   "execution_count": 127,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "most_human_like_tweet(\n",
    "    generated_tweets=generated_tweets,\n",
    "    benchmark=human_distribution_benchmark\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "iRZzTZL3FMKf",
    "outputId": "e2f133bb-62ae-4497-af49-fcf8ad959585"
   },
   "execution_count": 128,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tweet: I don‚Äôt know how many times people have told me ‚ÄúChatGPT is going to take over the world‚Äù and now it seems like they were right. #chatgptiscontrollingus üò≠üòÇü§£üíÄ‚ùóÔ∏è#fuckyouallofu\n",
      "Human-likeness Score: 0.9681\n",
      "Tweet: I'm not sure if this was an accident or intentional but the last few days have been filled with people saying that ChatGpt and other AI are taking over. It seems like it has become more of our reality than we thought possible.. #TheMatrixRebornIsReal\n",
      "Human-likeness Score: 0.9163\n",
      "Tweet: I'm not sure if it was the Chaos or something else, but there were some really weird things happening to me lately. It felt like my brain had been hacked and someone took control of what i thought/felt - especially when writing this article for example: https://tapas-hubbard\n",
      "Human-likeness Score: 0.9327\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'I don‚Äôt know how many times people have told me ‚ÄúChatGPT is going to take over the world‚Äù and now it seems like they were right. #chatgptiscontrollingus üò≠üòÇü§£üíÄ‚ùóÔ∏è#fuckyouallofu'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 128
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ex = {\n",
    "#    \"NNP\":0.11784279716897358,\n",
    "#    \"CC\":0.024218400664049265,\n",
    "#    \"RB\":0.05018358342244844,\n",
    "#    \"PRP\":0.056776967869437235,\n",
    "#    \"VBP\":0.03452376004040725,\n",
    "#    \"VBN\":0.011762690558332924,\n",
    "#    \"NN\":0.14094017384688828,\n",
    "#    \"VB\":0.04239662332460446,\n",
    "#    \"DT\":0.060314617271074934,\n",
    "#    \"IN\":0.07831549141786862,\n",
    "#    \"CD\":0.01759075570872018,\n",
    "#    \"NNS\":0.030624108061673955,\n",
    "#    \".\":0.06126383488450304,\n",
    "#    \"JJ\":0.05860965315014038,\n",
    "#    \"''\":0.006541993684478352,\n",
    "#    \",\":0.01911081385081816,\n",
    "#    \"WDT\":0.0016926202613119507,\n",
    "#    \"VBZ\":0.02524848315378056,\n",
    "#    \"POS\":0.00376336569187963,\n",
    "#    \"TO\":0.022043614130948196,\n",
    "#    \"``\":0.003658065011833731,\n",
    "#    \"PRP$\":0.015349715398461049,\n",
    "#    \"VBD\":0.02209576063996614,\n",
    "#    \"JJS\":0.0018689611130634563,\n",
    "#    \"WRB\":0.005254705466692362,\n",
    "#    \"MD\":0.012410617230768455,\n",
    "#    \"#\":0.013422561804319595,\n",
    "#    \"VBG\":0.02162694588980964,\n",
    "#    \"WP\":0.0037767172135122437,\n",
    "#    \"PDT\":0.000785472536801706,\n",
    "#    \"RP\":0.005473116207361727,\n",
    "#    \"RBR\":0.0009023613299627039,\n",
    "#    \"EX\":0.000996325812396006,\n",
    "#    \"(\":0.002306538340909692,\n",
    "#    \")\":0.002655693227377673,\n",
    "#    \":\":0.017687491261681006,\n",
    "#    \"JJR\":0.0020518517678800178,\n",
    "#    \"$\":0.0006375981368329435,\n",
    "#    \"FW\":0.00042221038219360463,\n",
    "#    \"SYM\":9.421639793580438e-05,\n",
    "#    \"NNPS\":0.0012192710321104095,\n",
    "#    \"RBS\":0.0003020466875000787,\n",
    "#    \"UH\":0.001199873538417744,\n",
    "#    \"WP$\":2.8718367285245185e-05,\n",
    "#    \"LS\":8.817042587575275e-06,\n",
    "#    \"NP\":0.7006743969661854,\n",
    "#    \"ADVP\":0.12389189727593185,\n",
    "#    \"ADJP\":0.05483301629873648,\n",
    "#    \"PP\":0.11205776103122488,\n",
    "#    \"VP\":0.008542928427921394\n",
    "# }"
   ],
   "metadata": {
    "id": "zh9MHFolFbi7"
   },
   "execution_count": 88,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "ayClfX6cKuIL"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
